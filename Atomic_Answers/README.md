# ğŸ§  Atomic Answers â€“ A RAG-based Assistant on *Atomic Habits*

**Atomic Answers** is a Retrieval-Augmented Generation (RAG) application built to answer habit-building questions using the content of *Atomic Habits* by James Clear. It uses OpenAI for embedding, Groq for generation (LLaMA3), and ChromaDB as the vector store.

Built with LangChain and Streamlit, this project demonstrates a full-stack, end-to-end RAG pipeline.

---

## ğŸ’¡ Features

- ğŸ“š Extracts and cleans PDF book into structured Markdown
- ğŸ”— Chunks content using LangChain's `RecursiveCharacterTextSplitter`
- ğŸ§  Embeds chunks using OpenAIâ€™s `text-embedding-3-small`
- ğŸ” Stores vectors in ChromaDB for similarity search
- ğŸ¤– Uses Groq (LLaMA3-8b-8192) to generate final answers
- ğŸŒ Clean and simple web UI with Streamlit

---

## ğŸ› ï¸ Tech Stack

| Layer             | Tool                      |
|------------------|---------------------------|
| Embeddings        | OpenAI                    |
| Generation        | Groq (LLaMA3-8b-8192)      |
| Vector Store      | ChromaDB                  |
| Framework         | LangChain (modular)       |
| UI                | Streamlit                 |
| Language          | Python                    |

---

## ğŸ“ Project Structure

```
Atomic_Answers/
â”œâ”€â”€ app.py                      # Streamlit web app
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ atomic_habits_full.md   # Full book in markdown
â”‚   â”œâ”€â”€ chapters/               # Chapter-wise markdown files
â”‚   â”œâ”€â”€ chunks_langchain.jsonl  # Chunks for embedding
â”‚   â””â”€â”€ embeddings_openai.jsonl # Chunk + embedding JSONL
â”œâ”€â”€ chroma/                     # Local vector DB (auto-created)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pdf_to_md.py
â”‚   â”œâ”€â”€ split_md_into_chapters.py
â”‚   â”œâ”€â”€ chunk_markdown_langchain.py
â”‚   â”œâ”€â”€ build_chroma_store.py
â”‚   â”œâ”€â”€ test_chroma_query.py
â”‚   â””â”€â”€ rag_ask_groq.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Run Locally

1. **Clone the repo**
```bash
git clone https://github.com/yourusername/atomic-answers.git
cd atomic-answers
```

2. **Create and activate virtual environment**
```bash
python -m venv .venv-rag
source .venv-rag/bin/activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Add your API keys to a `.env` file**
```env
OPENAI_API_KEY=your_openai_key
GROQ_API_KEY=your_groq_key
```

5. **Run the app**
```bash
streamlit run app.py
```

---

## âš™ï¸ How It Works

1. ğŸ“„ Book content is converted from PDF to Markdown
2. âœ‚ï¸ Markdown is split into overlapping chunks
3. ğŸ§  Embeddings are generated via OpenAI
4. ğŸ“¦ Stored in Chroma for vector-based retrieval
5. ğŸ—£ï¸ Question is matched with top chunks
6. ğŸ¤– Groq (LLaMA3) generates a final answer using the context

---

## ğŸ”® Future Enhancements

- Add support for multiple books or document sources
- Implement multi-turn chat or conversation history
- Add citation UI for each retrieved chunk
- Switch to local or fine-tuned embedding models for cost savings

---

## ğŸ“„ License

MIT â€” free to use, remix, and share.
